# -*- coding: utf-8 -*-
"""DIKAI_Colab_Full.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yBulsaDVx_dcwEWRinrlIcjzsfI4ip5p
"""

"""
DIKAI - Complete Google Colab Setup
Daystar Institutional Knowledge AI with MongoDB Vector Database

Run each cell in order:
1. Install Dependencies
2. Setup Configuration
3. Document Ingestion (Optional - if you need to add more data)
4. Backend API (FastAPI)
5. Frontend (Streamlit)
6. Launch Everything

Features:
- MongoDB Atlas Vector Database
- Multi-model AI (Gemini/Claude/Ollama)
- Document ingestion (PDF, DOCX, XLSX, Web scraping)
- Bilingual support (English/Swahili)
- Source citations
- Conversation memory
- ngrok tunnel
"""

# ============================================================
# CELL 1: Install Dependencies
# ============================================================
print("üì¶ Installing dependencies...")
!pip install -q -U \
  fastapi uvicorn streamlit python-dotenv pyngrok \
  pymongo \
  langchain langchain-core langchain-community \
  langchain-google-genai \
  langchain-huggingface \
  langchain-mongodb \
  sentence-transformers \
  beautifulsoup4 \
  pypdf \
  docx2txt \
  openpyxl \
  pandas \
  anthropic \
  nest_asyncio

import nest_asyncio
nest_asyncio.apply()

# Kill any existing processes
import subprocess
subprocess.run(["pkill", "-f", "uvicorn"], stderr=subprocess.DEVNULL)
subprocess.run(["pkill", "-f", "streamlit"], stderr=subprocess.DEVNULL)

print("‚úÖ Dependencies installed!")

# ============================================================
# CELL 2: Configuration & Setup
# ============================================================
import os
from pyngrok import ngrok

# Set your credentials
os.environ["MONGO_URI"] = "mongodb+srv://west:nowaybro@clustertest.bwlnpta.mongodb.net/?appName=Clustertest"
os.environ["GOOGLE_API_KEY"] = "AIzaSyD1UOmtxIT3gIXFYmCa_3GGHVymuzuCE8s"
os.environ["NGROK_TOKEN"] = "38exFrLkoXPGv48YkN2P5CHK4I2_4UefEgj2dTSCngZH7hw4U"

# Set ngrok auth token
ngrok.set_auth_token(os.environ["NGROK_TOKEN"])

# Database configuration
DB_NAME = "dikai_memory"
COLLECTION_NAME = "institutional_knowledge"
INDEX_NAME = "vector_index"
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# AI Model Selection - CHANGE THIS
AI_MODEL = "gemini"  # Options: "gemini", "claude", "ollama"

# If using Claude, set API key
if AI_MODEL == "claude":
    os.environ["ANTHROPIC_API_KEY"] = input("Enter your Anthropic API Key: ")

print("‚úÖ Configuration set!")
print(f"ü§ñ Using AI Model: {AI_MODEL.upper()}")

# Test MongoDB connection
from pymongo import MongoClient
try:
    client = MongoClient(os.environ["MONGO_URI"])
    client.admin.command('ping')
    print("‚úÖ MongoDB connected successfully!")
except Exception as e:
    print(f"‚ùå MongoDB connection failed: {e}")

# ============================================================
# CELL 3: Document Ingestion System (Optional)
# ============================================================
print("üìö Creating Document Ingestion System...")

import requests
import tempfile
import pandas as pd
from typing import List
from urllib.parse import urljoin
from bs4 import BeautifulSoup as Soup
from langchain_community.document_loaders import RecursiveUrlLoader, PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import MongoDBAtlasVectorSearch
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document

class DIKAI_Ingestor:
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}
        )
        self.client = MongoClient(os.environ["MONGO_URI"])
        self.collection = self.client[DB_NAME][COLLECTION_NAME]

    def clean_html(self, html: str) -> str:
        soup = Soup(html, "html.parser")
        for tag in soup(["nav", "footer", "aside", "script", "style", "header"]):
            tag.decompose()
        return " ".join(soup.get_text().split())

    def load_excel(self, file_path, source_url):
        docs = []
        try:
            xls = pd.read_excel(file_path, sheet_name=None)
            for sheet_name, df in xls.items():
                df = df.fillna("")
                for _, row in df.iterrows():
                    row_text = f"Sheet: {sheet_name}. " + ", ".join([
                        f"{col}: {val}" for col, val in row.items() if val != ""
                    ])
                    if len(row_text) > 20:
                        docs.append(Document(
                            page_content=row_text,
                            metadata={"source": source_url, "type": "excel"}
                        ))
        except Exception as e:
            print(f"‚ùå Excel Error: {e}")
        return docs

    def process_file(self, file_url):
        try:
            ext = file_url.lower().split('.')[-1]
            print(f"‚¨áÔ∏è  Downloading {ext.upper()}: {file_url}...")

            response = requests.get(file_url, timeout=15)
            response.raise_for_status()

            with tempfile.NamedTemporaryFile(delete=True, suffix=f".{ext}") as temp_file:
                temp_file.write(response.content)
                temp_file.flush()

                docs = []
                if ext == 'pdf':
                    loader = PyPDFLoader(temp_file.name)
                    docs = loader.load()
                elif ext in ['docx', 'doc']:
                    loader = Docx2txtLoader(temp_file.name)
                    docs = loader.load()
                elif ext in ['xlsx', 'xls']:
                    docs = self.load_excel(temp_file.name, file_url)

                for doc in docs:
                    doc.metadata["source"] = file_url

                print(f"‚úÖ Extracted {len(docs)} chunks from {ext.upper()}")
                return docs
        except Exception as e:
            print(f"‚ùå Failed to process {file_url}: {e}")
            return []

    def scrape_everything(self, urls: List[str]):
        all_docs = []
        file_links = set()
        TARGET_EXTENSIONS = ('.pdf', '.docx', '.doc', '.xlsx', '.xls')

        print("üïµÔ∏è  PHASE 1: Scraping HTML & Hunting Files...")
        for url in urls:
            print(f"   - Scanning: {url}")
            try:
                loader = RecursiveUrlLoader(
                    url=url,
                    max_depth=1,
                    extractor=self.clean_html,
                    exclude_dirs=['.*\\.pdf', '.*\\.docx', '.*\\.xlsx']
                )
                html_docs = loader.load()
                all_docs.extend(html_docs)

                for doc in html_docs:
                    try:
                        page_content = requests.get(doc.metadata['source'], timeout=10).content
                        soup = Soup(page_content, "html.parser")
                        for link in soup.find_all('a', href=True):
                            href = link['href']
                            if href.lower().endswith(TARGET_EXTENSIONS):
                                full_url = urljoin(doc.metadata['source'], href)
                                file_links.add(full_url)
                    except:
                        pass
            except Exception as e:
                print(f"‚ö†Ô∏è Error scanning {url}: {e}")

        print(f"üïµÔ∏è  PHASE 2: Processing {len(file_links)} Detected Files...")
        for file_url in file_links:
            file_docs = self.process_file(file_url)
            all_docs.extend(file_docs)

        return all_docs

    def ingest(self, documents):
        if not documents:
            print("‚ö†Ô∏è  No documents found.")
            return

        print(f"‚úÇÔ∏è  Splitting {len(documents)} items...")
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = splitter.split_documents(documents)

        print(f"üíæ Uploading {len(chunks)} vectors to MongoDB...")
        MongoDBAtlasVectorSearch.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            collection=self.collection,
            index_name=INDEX_NAME
        )
        print("üéâ SUCCESS: Documents ingested!")

print("‚úÖ Ingestion system ready!")

# Uncomment and run this to ingest new data:
# TARGET_URLS = [
#     "https://www.daystar.ac.ke/downloads.html",
#     "https://www.daystar.ac.ke/policies/"
# ]
# ingestor = DIKAI_Ingestor()
# docs = ingestor.scrape_everything(TARGET_URLS)
# ingestor.ingest(docs)

# ============================================================
# CELL 4: Create Backend API (api.py)
# ============================================================
# ============================================================
# CELL 4: Create Backend API (api.py)
# ============================================================
print("üîß Creating Backend API...")

# 1. Define the LLM code block based on Cell 2's selection
if AI_MODEL == "gemini":
    llm_setup_code = f"""
from langchain_google_genai import ChatGoogleGenerativeAI
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.3,
    google_api_key="{os.environ['GOOGLE_API_KEY']}"
)
"""
elif AI_MODEL == "claude":
    llm_setup_code = f"""
from langchain_anthropic import ChatAnthropic
llm = ChatAnthropic(
    model="claude-3-sonnet-20240229",
    temperature=0.3,
    anthropic_api_key="{os.environ.get('ANTHROPIC_API_KEY', '')}"
)
"""
elif AI_MODEL == "ollama":
    llm_setup_code = f"""
from langchain_community.chat_models import ChatOllama
llm = ChatOllama(model="llama3")
"""

api_code = f"""
import os
import uvicorn
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
from pymongo import MongoClient

# LangChain imports
from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter

# [Dynamic LLM Import]
{llm_setup_code}

# Configuration
MONGO_URI = "{os.environ['MONGO_URI']}"
DB_NAME = "{DB_NAME}"
COLLECTION_NAME = "{COLLECTION_NAME}"
INDEX_NAME = "{INDEX_NAME}"
EMBEDDING_MODEL = "{EMBEDDING_MODEL}"

app = FastAPI(title="DIKAI API", version="2.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Setup embeddings
embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL,
    model_kwargs={{"device": "cpu"}}
)

# Setup vector store
client = MongoClient(MONGO_URI)
vector_store = MongoDBAtlasVectorSearch(
    collection=client[DB_NAME][COLLECTION_NAME],
    embedding=embeddings,
    index_name=INDEX_NAME
)

retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={{"k": 10}}
)

# Bilingual prompt template
template = \"\"\"You are DIKAI (Daystar Institutional Knowledge AI).
You can understand and respond in both English and Swahili.
Use the following context to answer the question.
Always cite your sources.

Previous conversation:
{{chat_history}}

Context:
{{context}}

Question: {{question}}

Answer (in the same language as the question):\"\"\"

prompt = ChatPromptTemplate.from_template(template)

# Format documents helper
def format_docs(docs):
    formatted = []
    for i, doc in enumerate(docs):
        source = doc.metadata.get('source', 'Unknown')
        content = doc.page_content[:500]
        formatted.append(f"[Source {{i+1}}: {{source}}]\\n{{content}}\\n")
    return "\\n---\\n".join(formatted)

# --- FIXED CHAIN DEFINITION ---
# The logic here is:
# 1. Grab 'question' from input dict -> pass to retriever
# 2. Grab 'question' from input dict -> pass to prompt
# 3. Grab 'chat_history' -> pass to prompt
rag_chain = (
    {{
        "context": (lambda x: x["question"]) | retriever | format_docs,
        "question": lambda x: x["question"],
        "chat_history": lambda x: x.get("chat_history", "")
    }}
    | prompt
    | llm
    | StrOutputParser()
)

conversations = {{}}

class QueryRequest(BaseModel):
    question: str
    session_id: Optional[str] = "default"
    language: Optional[str] = "en"

class QueryResponse(BaseModel):
    answer: str
    sources: List[str]
    session_id: str

@app.post("/ask", response_model=QueryResponse)
async def ask_dikai(request: QueryRequest):
    try:
        session_id = request.session_id
        if session_id not in conversations:
            conversations[session_id] = []

        # Build chat history string
        chat_history = "\\n".join([
            f"User: {{msg['question']}}\\nDIKAI: {{msg['answer']}}"
            for msg in conversations[session_id][-3:]
        ])

        # Manually fetch sources for the UI display
        docs = retriever.invoke(request.question)
        sources = list(set([doc.metadata.get('source', 'Unknown') for doc in docs]))

        # Invoke chain
        # The chain now correctly handles the dictionary input because of the lambda fixes above
        answer = rag_chain.invoke({{
            "question": request.question,
            "chat_history": chat_history
        }})

        conversations[session_id].append({{
            "question": request.question,
            "answer": answer
        }})

        return QueryResponse(
            answer=answer,
            sources=sources[:5],
            session_id=session_id
        )
    except Exception as e:
        print(f"Error: {{e}}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ingest-url")
async def ingest_url(url: str):
    try:
        from langchain_community.document_loaders import RecursiveUrlLoader
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        from bs4 import BeautifulSoup as Soup

        def clean_html(html):
            soup = Soup(html, "html.parser")
            for tag in soup(["nav", "footer", "aside", "script", "style", "header"]):
                tag.decompose()
            return " ".join(soup.get_text().split())

        loader = RecursiveUrlLoader(url=url, max_depth=1, extractor=clean_html)
        docs = loader.load()

        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = splitter.split_documents(docs)

        MongoDBAtlasVectorSearch.from_documents(
            documents=chunks,
            embedding=embeddings,
            collection=client[DB_NAME][COLLECTION_NAME],
            index_name=INDEX_NAME
        )
        return {{"message": f"Successfully ingested {{len(chunks)}} chunks"}}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    return {{"status": "healthy"}}

@app.delete("/clear-history")
async def clear_history(session_id: str = "default"):
    if session_id in conversations:
        del conversations[session_id]
    return {{"message": "Cleared"}}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
"""

with open("api.py", "w") as f:
    f.write(api_code)

print("‚úÖ Backend API created (api.py)")

# ============================================================
# CELL 5: Create Frontend (app.py)
# ============================================================
print("üé® Creating Frontend...")

frontend_code = """
import streamlit as st
import requests
import uuid

st.set_page_config(
    page_title="DIKAI - Daystar AI",
    page_icon="üéì",
    layout="wide"
)

# Custom CSS
st.markdown(\"\"\"
<style>
    .stApp {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    }
    .main-header {
        text-align: center;
        color: white;
        padding: 20px;
        background: rgba(255, 255, 255, 0.1);
        border-radius: 10px;
        margin-bottom: 20px;
    }
    .chat-message {
        padding: 15px;
        border-radius: 10px;
        margin: 10px 0;
    }
    .user-message {
        background: rgba(255, 255, 255, 0.9);
        text-align: right;
    }
    .assistant-message {
        background: rgba(102, 126, 234, 0.2);
        color: white;
    }
</style>
\"\"\", unsafe_allow_html=True)

# Header
st.markdown(\"\"\"
<div class="main-header">
    <h1>üéì DIKAI</h1>
    <p>Daystar Institutional Knowledge AI</p>
    <p><i>Ask anything about Daystar University policies, procedures, and regulations</i></p>
</div>
\"\"\", unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    st.title("‚öôÔ∏è Settings")

    language = st.selectbox(
        "Language / Lugha",
        ["English", "Swahili"],
        help="Choose your preferred language"
    )

    st.divider()

    st.subheader("üìö Add New Content")
    new_url = st.text_input("Enter URL to scrape:")
    if st.button("Add URL to Knowledge Base"):
        if new_url:
            with st.spinner("Ingesting content..."):
                try:
                    response = requests.post(
                        "http://127.0.0.1:8000/ingest-url",
                        params={"url": new_url},
                        timeout=120
                    )
                    if response.status_code == 200:
                        st.success("‚úÖ Content added successfully!")
                    else:
                        st.error(f"Error: {response.text}")
                except Exception as e:
                    st.error(f"Failed to add content: {e}")

    st.divider()

    if st.button("üóëÔ∏è Clear Chat History"):
        st.session_state.messages = []
        st.session_state.session_id = str(uuid.uuid4())
        try:
            requests.delete(
                f"http://127.0.0.1:8000/clear-history",
                params={"session_id": st.session_state.session_id}
            )
        except:
            pass
        st.rerun()

    st.divider()
    st.caption("Made with ‚ù§Ô∏è for Daystar University")

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message and message["sources"]:
            with st.expander("üìÑ Sources"):
                for source in message["sources"]:
                    st.caption(f"‚Ä¢ {source}")

# Chat input
if prompt := st.chat_input(
    "Ask me anything... / Uliza swali lolote..." if language == "Swahili"
    else "Ask me anything about Daystar..."
):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Get AI response
    with st.chat_message("assistant"):
        with st.spinner("Thinking... / Ninafikiria..."):
            try:
                response = requests.post(
                    "http://127.0.0.1:8000/ask",
                    json={
                        "question": prompt,
                        "session_id": st.session_state.session_id,
                        "language": "sw" if language == "Swahili" else "en"
                    },
                    timeout=120
                )

                if response.status_code == 200:
                    data = response.json()
                    answer = data["answer"]
                    sources = data.get("sources", [])

                    st.markdown(answer)

                    if sources:
                        with st.expander("üìÑ Sources"):
                            for source in sources:
                                st.caption(f"‚Ä¢ {source}")

                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": answer,
                        "sources": sources
                    })
                else:
                    error_msg = f"‚ö†Ô∏è Error: {response.text}"
                    st.error(error_msg)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })
            except Exception as e:
                error_msg = f"‚ùå Connection failed: {e}"
                st.error(error_msg)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_msg
                })
"""

with open("app.py", "w") as f:
    f.write(frontend_code)

print("‚úÖ Frontend created (app.py)")

# ============================================================
# CELL 6: Launch Everything
# ============================================================
import threading
import time
import requests

print("üöÄ Launching DIKAI System...")

# Start Backend
def run_backend():
    os.system("python api.py > api.log 2>&1")

backend_thread = threading.Thread(target=run_backend, daemon=True)
backend_thread.start()

print("‚è≥ Starting backend...")
time.sleep(10)

# Check if backend is ready
backend_ready = False
for i in range(15):
    try:
        response = requests.get("http://127.0.0.1:8000/health", timeout=2)
        if response.status_code == 200:
            print("‚úÖ Backend is ONLINE!")
            backend_ready = True
            break
    except:
        time.sleep(3)
        print(f"   Waiting for backend... ({i+1}/15)")

if not backend_ready:
    print("‚ùå BACKEND FAILED TO START. Check logs:")
    if os.path.exists("api.log"):
        with open("api.log", "r") as f:
            print(f.read())
else:
    # Start Frontend
    print("‚è≥ Starting frontend...")
    def run_frontend():
        os.system("streamlit run app.py --server.port 8501 > streamlit.log 2>&1")

    frontend_thread = threading.Thread(target=run_frontend, daemon=True)
    frontend_thread.start()
    time.sleep(8)

    # Setup ngrok tunnel
    try:
        public_url = ngrok.connect(8501, bind_tls=True)

        print("\n" + "="*60)
        print("üéâ DIKAI IS LIVE!")
        print("="*60)
        print(f"üåê Public URL: {public_url}")
        print(f"üìö API Docs: http://127.0.0.1:8000/docs")
        print(f"‚ù§Ô∏è  Health: http://127.0.0.1:8000/health")
        print("="*60)
        print("üí° Click the URL above to access DIKAI")
        print("üí° Keep this notebook running!")
        print("="*60)

        # Keep alive
        print("\n‚è∞ System running... Press 'Stop' button to shutdown.")
        while True:
            time.sleep(60)

    except KeyboardInterrupt:
        print("\nüõë Shutting down...")
        ngrok.disconnect(public_url)
        print("‚úÖ Stopped!")
    except Exception as e:
        print(f"‚ùå Ngrok error: {e}")
        print("Backend still accessible at: http://127.0.0.1:8000/docs")
